{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import requests\n",
    "import logging\n",
    "import base64\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nature_go_client\n",
    "from summary_generation import generate_summaries\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_URL = 'http://nature-go.edouardleurent.com'\n",
    "LOGIN_URL = BASE_URL + '/api/auth/login/'\n",
    "SPECIES_LIST_URL = BASE_URL + '/api/species/all/'\n",
    "SPECIES_DETAIL_URL = BASE_URL + '/api/species/{species_id}/'\n",
    "QUESTION_CREATE_URL = BASE_URL + '/api/university/quiz/questions/'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Your id here\n",
    "NG_USERNAME = os.getenv(\"NG_USERNAME\")\n",
    "NG_PASSWORD = os.getenv(\"NG_PASSWORD\")\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = nature_go_client.NatureGoClient(username=NG_USERNAME, password=NG_PASSWORD)\n",
    "client.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = client.get_all_species(limit=100)\n",
    "\n",
    "species_without_summary = [e for e in species_list if e['descriptions'] == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 25/90 [06:28<17:30, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 6249 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 6249 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4374 tokens (3574 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4311 tokens (3511 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4311 tokens (3511 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4393 tokens (3593 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4311 tokens (3511 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4374 tokens (3574 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4374 tokens (3574 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4393 tokens (3593 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4393 tokens (3593 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4311 tokens (3511 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4311 tokens (3511 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4346 tokens (3546 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 29/90 [07:52<18:41, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4335 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4335 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 33/90 [08:53<14:54, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4309 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4312 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 34/90 [09:10<14:58, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 5175 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 5170 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3540 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4318 tokens (3518 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4381 tokens (3581 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4381 tokens (3581 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4397 tokens (3597 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 52/90 [14:10<11:03, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4384 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4389 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4354 tokens (3554 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [18:56<06:03, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4098 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4098 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4398 tokens (3598 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4376 tokens (3576 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4337 tokens (3537 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4376 tokens (3576 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4301 tokens (3501 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 74/90 [20:01<04:15, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4174 tokens (3374 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 79/90 [21:17<02:46, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4098 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, your messages resulted in 4103 tokens. Please reduce the length of the messages. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 86/90 [23:05<00:57, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4724 tokens (3924 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4619 tokens (3819 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4396 tokens (3596 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n",
      "API error occurred: This model's maximum context length is 4097 tokens. However, you requested 4380 tokens (3580 in the messages, 800 in the completion). Please reduce the length of the messages or completion. . Retrying in 1 second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [24:11<00:00, 16.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for species in tqdm(species_without_summary):\n",
    "    species_name, species_id = species['scientificNameWithoutAuthor'], species['id']\n",
    "    summaries = generate_summaries(species_name)\n",
    "    client.post_species_descriptions(species_id, summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730210 total words\n",
      "1.46042 total wiki cost\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(client.get_labeled_species(limit=1000, descriptions=False))\n",
    "\n",
    "print(df['wikipedia_word_count'].sum(), \"total words\")\n",
    "print(df['wikipedia_word_count'].sum() / 1000 * 0.002, \"total wiki cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
